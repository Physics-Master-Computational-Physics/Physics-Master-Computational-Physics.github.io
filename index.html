<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Elias Huber, Sam Rouppe Van der Voort" />
  <title>Seminar on Scientific Machine Learning: Likelihood-based Generative models: VAEs and NFs</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Seminar on Scientific Machine Learning:<br />
Likelihood-based Generative models: VAEs and NFs</h1>
<p class="author">Elias Huber, Sam Rouppe Van der Voort</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<p>This Document summarizes the Seminar talk on "Likelihood-based
Generative Models: VAEs and NFs" of the Scientific Machine Learning
Seminar. Here, we first introduce Likelihood-based models in a general
scope. We then continue with deriving the theory behind Variational
Autoencoders and Normalizing Flows. We also give small example
implementations for both to give a proof of principle. We then proceed
with two experimental tasks, where we use the presented methods to train
models for learning the distribution generated by a QR-code and an image
generation model that uses Normalizing Flows and Variational
Autoencoders together to generate and classify images of fruit. Finally,
we conclude our presentation by discussing the possible applications in
physics. All code examples can be found on our GitHub repository <span
class="citation" data-cites="GitHub"></span></p>
</div>
</header>
<h1 id="introduction">Introduction</h1>
<p>In recent years, Likelihood-based generative models have had a
significant impact in fields like computer vision <span class="citation"
data-cites="brubaker2021tutorial"></span> and natural language
processing <span class="citation"
data-cites="zhao2025surveylargelanguagemodels"></span>. Two foundational
methods of this field are Variational Autoencoders (VAEs) <span
class="citation"
data-cites="kingma2013autoencoding kingma2019introduction"></span> and
Normalizing Flows (NFs) <span class="citation"
data-cites="rezende2015variational papamakarios2021normalizing"></span>.<br />
In this article, we will present and discuss both of these methods,
provide implementation examples, and discuss their applications in
physics.</p>
<h1 id="likelihood-based-generative-modeling">Likelihood-based
Generative Modeling</h1>
<p>Likelihood-based generative modeling deals with the issue of
generating samples that match the characteristics of a given dataset.
Mathematically, we have given a dataset <span
class="math inline">\(\mathbf{X} = ( \mathbf{x}_{1}, \mathbf{x}_2,
\ldots, \mathbf{x}_{n} )\)</span> that follows an underlying data
distribution <span class="math inline">\(p(\mathbf{x})\)</span>. To
sample and evaluate new data, we want to find a distribution <span
class="math inline">\(p_\theta(\mathbf{x})\)</span> that approximates
the underlying true distribution <span
class="math inline">\(p_(\mathbf{x})\)</span> that the data follows. For
this, we assume some approximation function <span
class="math inline">\(p_\theta(\mathbf{x})\)</span> and then optimize
the selection of the parameter <span
class="math inline">\(\theta\)</span> such that it matches the original
distribution the best. This is done by maximizing the likelihood <span
class="math display">\[\begin{aligned}
     \mathcal{L}(\theta) = \prod_{i=1}^{n}
p_\theta(\mathbf{x}_i)\end{aligned}\]</span> that we observe our data,
given our approximation of the probability distribution <span
class="math inline">\(p_\theta(\mathbf{x})\)</span> with parameters
<span class="math inline">\(\theta\)</span>. In practice, we model our
approximation distribution <span
class="math inline">\(p_\theta(\mathbf{x})\)</span> with neural networks
and minimize the negative log-likelihood <span
class="math display">\[\begin{aligned}
    \hat{\theta}_{MLE} &amp;=  \arg \min_{\theta} J(\theta,
\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(n)}) \\
    &amp;= -\frac{1}{n} \sum_{i=1}^{n}\log
p_{\theta}(\mathbf{x}^{(i)}),\end{aligned}\]</span> to find the optimal
weights of the neural network. We minimize the log-likelihood due to
numerical stability. The parameter optimization process is then done
using gradient descent-based algorithms.</p>
<h1 id="variational-autoencoders">Variational Autoencoders</h1>
<h2 id="theory">Theory</h2>
<p>Variational Autoencoders (abbreviated VAEs) introduced by Kingma et
al in <span class="citation"
data-cites="kingma2013autoencoding"></span>, are conceptually similar to
Autoencoders (abbreviated AE). The main idea is to encode data in a
lower-dimensional latent space and then to recreate or create new data
from the latent space. Practically, both architectures achieve this by
consisting of an encoder and a decoder. Unlike AEs, which encode each
data point to a point in the latent space, VAEs encode each data point
to a distribution in the latent space. This is achieved by the encoder
and decoder approximating conditional probability distributions. In
particular the probability of a latent state given a data point <span
class="math inline">\(p(z|x)\)</span>, and conversely the probability of
a data point given a latent state <span
class="math inline">\(p(x|z)\)</span>. For VAEs both distributions are
approximated with neural networks <span class="math inline">\(p(z|x)
\approx q_\phi(z|x)\)</span> for the encoder and <span
class="math inline">\(p(x|z)\)</span> for the decoder we approximate
<span class="math inline">\(p(x|z) \approx p_\theta(x|z)\)</span> with
parameters <span class="math inline">\(\phi\)</span> and <span
class="math inline">\(\theta\)</span> respectively. For the latent space
to be a distribution <span class="math inline">\(q_\phi(z|x)\)</span>
outputs the parameters of the distribution of the latent space. Commonly
gaussian with a mean vector <span
class="math inline">\(\mu_{\phi}\)</span> and a standard deviation
vector <span class="math inline">\(\sigma_{\phi}\)</span>.</p>
<h3 id="elbo">ELBO</h3>
<p>As discussed before VAEs are a subclass of likelihood based modeling.
So we want to maximize the log-likelihood to obtain the loss function
for the VAEs. An exact solution is not tractable but we can obtain a
lower bound. This results in the ELBO, the loss function to achieve the
approximations of the conditional probability distributions. The ELBO is
the expectation value, wrt. samples from the encoder probability
distribution <span class="math inline">\(q_{\phi}\)</span> expressed as
follows: <span class="math display">\[\begin{aligned}
    &amp;\text{ELBO}(x) = \mathbb{E}_{q_\phi(z|x)} \left[ \log
\frac{p(x, z)}{q_\phi(z|x)} \right] \\
    &amp;= \underbrace{\mathbb{E}_{q_\phi(z|x)} \left[ \log
p_\theta(x|z) \right]}_{\text{Reconstruction}}
    - \underbrace{D_{\text{KL}}(q_\phi(z|x) \,\|\,
p(z))}_{\text{Regularizing}}.\end{aligned}\]</span></p>
<p>We see that the ELBO can be split up into a reconstruction term and a
regularizing term. The reconstruction term should ensure a good
reconstruction and depends on the assumed distribution of the target.
For example, the reconstruction term becomes the MSE for gaussian
assumption or BCE loss for Bernoulli assumption. While the regularizing
term enforces the encoder distribution to resemble the assumed prior in
latent space, which for the vanilla VAE case is standard gaussian.</p>
<h3 id="reparameterization-trick">Reparameterization trick</h3>
<p>For calculating the ELBOs reconstruction term in practice <span
class="math inline">\(\mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x|z)
\right]\)</span>, we rely on samples <span
class="math inline">\(z\)</span> from the learned distribution <span
class="math inline">\(q_\phi(z|x)\)</span>. The sampling however is not
differentiable wrt. <span class="math inline">\(\phi\)</span>. To
circumvent this problem, Kingma et al. <span class="citation"
data-cites="kingma2013autoencoding"></span>) proposed a so called
reparameterization trick. The transformation <span
class="math inline">\(z = \mu_\phi + \sigma_\phi \odot\epsilon\)</span>
in terms of the distribution parameters <span
class="math inline">\(\mu_\phi\)</span> and <span
class="math inline">\(\sigma_\phi\)</span> from <span
class="math inline">\(q_\phi(z|x)\)</span> makes the sampling
independent of <span class="math inline">\(\phi\)</span>. The stochastic
variable <span class="math inline">\(\epsilon\)</span> is sampled form a
standard normal distribution <span
class="math inline">\(\mathcal{N}(0,\text{I})\)</span> and due to the
properties of Gaussians, the distribution of <span
class="math inline">\(z(\mu_\phi, \sigma_\phi,\epsilon)\)</span> will
still be normal <span
class="math inline">\(\mathcal{N}(\mu_\phi,\sigma_\phi)\)</span>. So the
distribution of <span class="math inline">\(z\)</span> remains unchanged
and the transformation allows samples of <span
class="math inline">\(z\)</span> to be differentiated wrt. <span
class="math inline">\(\phi\)</span> since the stochasticity now is in
<span class="math inline">\(\epsilon\)</span>.</p>
<h2 id="implementation-mnist">Implementation: MNIST</h2>
<p>For a showcase of VAEs we implemented a VAE for generating new
MNIST-like images. We did this with torch by using feedforward neural
networks for the encoder and decoder networks, approximating <span
class="math inline">\(q_\phi(z|x)\)</span> and <span
class="math inline">\(p_\theta(x|z)\)</span>. We choose a latent
dimension of 2 for visualization shown in Figure <a
href="#fig:latentMNIST" data-reference-type="ref"
data-reference="fig:latentMNIST">1</a>.</p>
<figure>
<img src="images/MNISTlatentdistrib.png" id="fig:latentMNIST"
alt="Latent distribution of encoded images, colored by label." />
<figcaption aria-hidden="true">Latent distribution of encoded images,
colored by label.</figcaption>
</figure>
<p>Further more, we explored the continuity of the latent space by
interpolating between two encoded images, decoding along the way. The
result is shown in Figure <a href="#fig:MNISTint"
data-reference-type="ref" data-reference="fig:MNISTint">2</a> or as Gif
on our GitHub repository <span class="citation"
data-cites="GitHub"></span>.</p>
<figure>
<img src="assets/MNIST.gif" id="fig:MNISTint"
alt="Linear interpolation between MNIST images." />
<figcaption aria-hidden="true">Linear interpolation between MNIST
images.</figcaption>
</figure>
<p>Finally, we also generated new MNIST-like images, see Figure <a
href="#fig:genMNIST" data-reference-type="ref"
data-reference="fig:genMNIST">3</a>.</p>
<figure>
<img src="images/GenMNIST.png" id="fig:genMNIST"
alt="Novel MNIST images generated with trained VAE." />
<figcaption aria-hidden="true">Novel MNIST images generated with trained
VAE.</figcaption>
</figure>
<h1 id="normalizing-flows">Normalizing Flows</h1>
<h2 id="theory-1">Theory</h2>
<p>Normalizing flows have a different approach than VAEs presented
before. Instead of learning our target distribution directly, we learn a
transformation <span class="math inline">\(f\)</span> from a base
distribution <span class="math inline">\(p_Z(\textbf{z})\)</span> to our
target distribution <span
class="math inline">\(p_X(\textbf{x})\)</span>. This enables us to
efficiently sample from our target distribution, by sampling from the
base distribution and transforming the sample to the target space.
Mathematically, we utilize the change of variables formula <span
class="math display">\[\begin{aligned}
     p_X(\textbf{x}) = p_Z(f(\textbf{x})) \left| \det J_f(\textbf{x})
\right|^{-1} \end{aligned}\]</span> to find our transformation. Here,
<span class="math inline">\(f(\textbf{x})\)</span> is a bijective
function <span
class="math inline">\(f:\mathbb{R}^d\rightarrow\mathbb{R}^d\)</span>
that transforms both equally dimensional probability spaces into each
other. The significant part of this equation is the calculation of the
normalization constant, given by the determinant of the Jacobian matrix
of the transform <span class="math inline">\(\left| \det J_f(\textbf{x})
\right|^{-1}\)</span>. This restrains our implementation of the
transformation function <span class="math inline">\(f(x)\)</span> to be
invertible and differentiable. We thus need to use invertible and fully
differentiable neural networks to implement the transformation.<br />
</p>
<h3 id="coupling-flows">Coupling Flows</h3>
<p>One prominent example of a normalizing flow implementation is the
so-called coupling flow. Here, we split our input data in two parts.
While we leave the first part of the input dimensions unchanged, we use
them as input to a neural network that outputs a transform that acts on
the second part. This neural network is also called a coupling network.
The schematic structure of such a coupling network can be seen in Figure
<a href="#fig:coupling_transform" data-reference-type="ref"
data-reference="fig:coupling_transform">4</a>. This kind of flow is
useful since the Jacobian of the transformation takes the form of a
lower triangular matrix, which reduces the complexity of the determinant
calculation from <span class="math inline">\(\mathcal{O}(d^3)\)</span>
to <span class="math inline">\(\mathcal{O}(d^2)\)</span>, where <span
class="math inline">\(d\)</span> is the dimensionality of the
distributions.</p>
<figure>
<img src="images/nf_coupling_flows.png" id="fig:coupling_transform"
alt="Schematic of a coupling flow ." />
<figcaption aria-hidden="true">Schematic of a coupling flow <span
class="citation" data-cites="brubaker2021tutorial"></span>.</figcaption>
</figure>
<h3 id="composition-of-flows">Composition of Flows</h3>
<p>To enhance the expressiveness of normalizing flows, it is possible to
concatenate multiple normalizing flow transforms, to divide the task of
finding the proper transform into multiple sub-tasks <span
class="math display">\[\begin{aligned}
    f = f_K \circ f_{K-1} \circ \cdots \circ f_1.\end{aligned}\]</span>
The determinant, necessary for normalization then can be calculated with
the relation <span class="math display">\[\begin{aligned}
    \det J_f = \det J_{f_K} \cdot \det J_{f_{K-1}} \cdots \det J_{f_1} =
\prod_{k=1}^{K} \det J_{f_k}.\end{aligned}\]</span> This trick greatly
increases the expressive power of normalizing flows while keeping
computational cost low, by implementing simple transforms like the
previously shown coupling flows and concatenating them to achieve higher
expressiveness.</p>
<h2 id="implementation">Implementation</h2>
<p>For a proof of concept, We used normalizing flows to learn a
double-peaked gaussian in two dimensions. For that, we defined the
standard 2D gaussian as the base distribution. For the normalizing flow,
we concatenated eight coupling transforms that use affine
transformations as coupling networks. The implementation was done in
python, using the nflows library with torch. The base, target and
generated distribtions are shown in Figures <a
href="#fig:nf_base_target_generated" data-reference-type="ref"
data-reference="fig:nf_base_target_generated">7</a>. The code can be
found in the notebook NFexample.</p>
<figure>
<img src="images/nf_example_gaussian_base.png"
id="fig:nf_base_target_generated"
alt="From left to right: Samples from base, target and generated distributions." />
<figcaption aria-hidden="true">Base distribution.</figcaption>
</figure>
<figure>
<img src="images/nf_example_gaussian_train.png"
id="fig:nf_base_target_generated"
alt="From left to right: Samples from base, target and generated distributions." />
<figcaption aria-hidden="true"> Target distribution.</figcaption>
</figure>
<figure>
<img src="images/nf_example_gaussian_generated.png"
id="fig:nf_base_target_generated"
alt="From left to right: Samples from base, target and generated distributions." />
<figcaption aria-hidden="true">Generated distribution.</figcaption>
</figure>
<h1 id="experiments">Experiments</h1>
<p>To test both of these methods, we implemented two case studies where
we tried to bring both methods to their limits.</p>
<h2 id="learning-the-distribution-of-a-qr-code">Learning the
distribution of a QR code</h2>
<p>In an effort to probe the expressiveness of normalizing flows, we
tried to learn the distribution that is obtained by transforming the
QR-Code of our Github repository into an uniform distribution (see
Figure <a href="#fig:qr_combined" data-reference-type="ref"
data-reference="fig:qr_combined">9</a>). We first tried to use different
types of normalizing flows to learn the transformation from a standard
2D Gaussian to the underlying distribution of the QR-Code. We observed
that the method failed to capture the fine structure of the QR code
pixels. We thus modified our approach to use a more advanced and
computationally more efficient approach to the problem: Flow matching.
Here, instead of finding discrete transforms, we try to find a
continuous velocity field that changes between the times 0 and 1, to
push our samples forward in time using an ODE integrator. Note that this
method has close ties with the previous seminar on neural ODEs. The
resulting generated distribution looked more like a QR code but was
still not scannable. The sharp edges of the pixels were limiting the
performance. We thus extended our transformation to also learn the
Fourier features of the target distribution, to increase the precision
for high frequency features like the sharp edges of the pixels. This led
to the ability to recreate the QR code distribution with high accuracy.
The result is shown in Figure <a href="#fig:qr_combined"
data-reference-type="ref" data-reference="fig:qr_combined">9</a>. An
animation of the Flow matching process can be found in our GitHub
repository <span class="citation" data-cites="GitHub"></span>.</p>
<figure>
<img src="assets/flow_animation.gif" id="fig:qr_combined"
alt="Left: Target QR code. Right: Generated samples using flow matching." />
<figcaption aria-hidden="true"> Generated samples using flow matching.</figcaption>
</figure>
<h2 id="image-generation-of-fruits">Image generation of Fruits</h2>
<p>For further showcase of VAEs we implemented a VAE for generation of
fruit images. We used a CNN-based architecture for the encoder and
decoder. The data used was around 1400 images from Kaggle <span
class="citation" data-cites="sshikamaru2020fruit"></span>. The three
classes of apples, kiwis and bananas had approximately equal share. The
generation of new samples was not sastisfying at first, so we tried to
improve our model by adding normalizing flows to the latent distribution
to improve the flexibility of the encoder <span
class="math inline">\(q_{\phi}\)</span>.</p>
<h3 id="adding-nfs-for-a-more-expressive-latent-space">Adding NFs for a
more expressive latent space</h3>
<p>The idea behind adding NFs, is that this approach lifts the
limitation of the latent space being gaussian, which thus increases
expressivity. Specifically we implemented inverse autoregressive flows
(IAFs), which were suggested for VAEs by Kingma in <span
class="citation" data-cites="kingma2016improving"></span>. The IAF flow
expression is <span class="math inline">\(z^{(t)} = \mu^{(t)}(z^{(t-1)})
+ \sigma^{(t)}(z^{(t-1)}) \odot z^{(t-1)}\)</span>. Notice that the form
of the transformation is similar to the reparameterization trick, here
however we perform flows up to T times, since we can compose multiple
flows. This flow formulation has desirable properties for a normalizing
flow as it is invertible and yields a triangular Jacobian. Visually, we
did not see an improvement, and propose that we need more data or a
pretrained visual model for the VAE to properly learn how to generate
fruit images.</p>
<h3 id="latent-space-exploration">Latent space exploration</h3>
<p>Similar to the example of MNIST, we explored the latent space of our
models. We did this for both the pure VAE with CNN architecture and the
VAE with additional normalizing flows in the latent space. To explore
the latent sapce, we linearly interpolated between encoded images. Here
the latent dimension however is much greater than for the MNIST example,
where we have set it to 128 and 64 respectively. An example of such a
latent space exploration can be found in Figure <a
href="#fig:banan-apple" data-reference-type="ref"
data-reference="fig:banan-apple">10</a>.</p>
<figure>
<img src="assets/apple_bannan_NF.gif" id="fig:banan-apple"
alt="Linear interpolation between Banana and Apple." />
<figcaption aria-hidden="true">Linear interpolation between Banana and
Apple.</figcaption>
</figure>
<p>Observing the latent sapce, we see that the VAE struggles with
recreating high quality, and coherent images. It seems however to still
be able to separate between fruit classes because when trying to sample
from the neighborhood of an encoded training data image, we yield
blurrier versions of the encoded image, not different fruits.</p>
<h1 id="applications-in-physics">Applications in Physics</h1>
<p>Likelihood-based generative models are used predominantly in two
different modes. We either want to generate new samples that weren’t in
the training set, or we want to evaluate the probability of new samples
we observe.</p>
<h2 id="generating-new-samples">Generating new samples</h2>
<p>VAEs and NFs are both used for generative models, even though they
are not state-of-the-art in some cases. Application fields include
chemical design <span class="citation"
data-cites="gomez-bombarelli2018automatic"></span> and generation of
rare galaxy images for telescope calibration <span class="citation"
data-cites="ravanbakhsh2016enabling"></span>.</p>
<h2 id="evaluating-samples">Evaluating samples</h2>
<p>Both VAEs and NFs are used to evaluate sample probabilities. Since
VAEs only evaluate the ELBO, which gives an lower bound, they are mainly
used for anomaly detection <span class="citation"
data-cites="pol2020anomaly"></span>. NFs can also be used to efficiently
evaluate more exact probabilities, as they are necessary for Markov
Chain Monte Carlo methods <span class="citation"
data-cites="roggero2021normalizing"></span>.</p>
<h1 id="summary">Summary</h1>
<p>We gave an introduction to likelihood based generative modeling,
discussing the mathematical framework as well as the concrete
application form of the problem. We then introduced VAEs and Normalizing
Flows, as prominent methods of likelihood based generative modeling.
Defining features of the methods being that VAEs approximate the target
distribution through an encoder decoder structure and Normalizing flows
learn a transformation from a simple distribution to the target
distribution. Our examples include learning the MNIST dataset and a 2D
two-peaked Gaussian. In an effort to test the limits of these approaches
we furthermore studied more challenging applications that include
learning the distribution of a QR code and learning the distribution
behind images of fruit. To solve these cases, we tired different
architectures and approches, combining knowledge from previous seminar
talks. Finally, we gave examples on where these methods are used in
scientific research in physics.</p>
</body>
</html>
